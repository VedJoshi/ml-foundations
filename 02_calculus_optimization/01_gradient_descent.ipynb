{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculus & Optimization for ML\n",
    "\n",
    "## Key Concepts\n",
    "- Gradients, Jacobians, Hessians\n",
    "- Chain rule in vector form\n",
    "- Gradient descent variants (SGD, momentum, Adam)\n",
    "- Convexity\n",
    "\n",
    "## References\n",
    "- Matrix Cookbook: Sections on derivatives\n",
    "- FOML: Optimization sections\n",
    "- CS231n: Optimization notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Gradient Descent\n",
    "\n",
    "Update rule:\n",
    "$$w_{t+1} = w_t - \\alpha \\nabla L(w_t)$$\n",
    "\n",
    "where $\\alpha$ is the learning rate and $\\nabla L$ is the gradient of the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(f, grad_f, x0, learning_rate=0.1, n_iters=100):\n",
    "    \"\"\"Basic gradient descent\n",
    "    \n",
    "    Args:\n",
    "        f: Objective function\n",
    "        grad_f: Gradient of objective function\n",
    "        x0: Initial point\n",
    "        learning_rate: Step size\n",
    "        n_iters: Number of iterations\n",
    "    \n",
    "    Returns:\n",
    "        x_history: List of x values\n",
    "        f_history: List of f(x) values\n",
    "    \"\"\"\n",
    "    x = x0.copy()\n",
    "    x_history = [x.copy()]\n",
    "    f_history = [f(x)]\n",
    "    \n",
    "    for _ in range(n_iters):\n",
    "        grad = grad_f(x)\n",
    "        x = x - learning_rate * grad\n",
    "        x_history.append(x.copy())\n",
    "        f_history.append(f(x))\n",
    "    \n",
    "    return np.array(x_history), np.array(f_history)\n",
    "\n",
    "# Test on quadratic: f(x) = x^T A x / 2\n",
    "A = np.array([[2, 0], [0, 1]])\n",
    "f = lambda x: 0.5 * x @ A @ x\n",
    "grad_f = lambda x: A @ x\n",
    "\n",
    "x0 = np.array([2.0, 2.0])\n",
    "x_hist, f_hist = gradient_descent(f, grad_f, x0, learning_rate=0.3, n_iters=20)\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(f_hist)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('f(x)')\n",
    "plt.title('Loss over iterations')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(x_hist[:, 0], x_hist[:, 1], 'o-')\n",
    "plt.xlabel('x1')\n",
    "plt.ylabel('x2')\n",
    "plt.title('Optimization path')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. SGD with Momentum\n",
    "\n",
    "Adds momentum to smooth updates:\n",
    "$$v_{t+1} = \\beta v_t + \\nabla L(w_t)$$\n",
    "$$w_{t+1} = w_t - \\alpha v_{t+1}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd_momentum(f, grad_f, x0, learning_rate=0.1, momentum=0.9, n_iters=100):\n",
    "    \"\"\"SGD with momentum\"\"\"\n",
    "    x = x0.copy()\n",
    "    v = np.zeros_like(x)\n",
    "    x_history = [x.copy()]\n",
    "    f_history = [f(x)]\n",
    "    \n",
    "    for _ in range(n_iters):\n",
    "        grad = grad_f(x)\n",
    "        v = momentum * v + grad\n",
    "        x = x - learning_rate * v\n",
    "        x_history.append(x.copy())\n",
    "        f_history.append(f(x))\n",
    "    \n",
    "    return np.array(x_history), np.array(f_history)\n",
    "\n",
    "# Compare with basic GD\n",
    "x_hist_mom, f_hist_mom = sgd_momentum(f, grad_f, x0, learning_rate=0.1, momentum=0.9, n_iters=20)\n",
    "\n",
    "plt.plot(f_hist, label='GD')\n",
    "plt.plot(f_hist_mom, label='SGD + Momentum')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('f(x)')\n",
    "plt.legend()\n",
    "plt.title('Comparison: GD vs Momentum')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Adam Optimizer\n",
    "\n",
    "Adaptive learning rates with momentum:\n",
    "$$m_t = \\beta_1 m_{t-1} + (1-\\beta_1) g_t$$\n",
    "$$v_t = \\beta_2 v_{t-1} + (1-\\beta_2) g_t^2$$\n",
    "$$\\hat{m}_t = m_t / (1-\\beta_1^t)$$\n",
    "$$\\hat{v}_t = v_t / (1-\\beta_2^t)$$\n",
    "$$w_t = w_{t-1} - \\alpha \\hat{m}_t / (\\sqrt{\\hat{v}_t} + \\epsilon)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adam(f, grad_f, x0, learning_rate=0.01, beta1=0.9, beta2=0.999, eps=1e-8, n_iters=100):\n",
    "    \"\"\"Adam optimizer\"\"\"\n",
    "    x = x0.copy()\n",
    "    m = np.zeros_like(x)\n",
    "    v = np.zeros_like(x)\n",
    "    x_history = [x.copy()]\n",
    "    f_history = [f(x)]\n",
    "    \n",
    "    for t in range(1, n_iters + 1):\n",
    "        grad = grad_f(x)\n",
    "        \n",
    "        # Update biased first moment\n",
    "        m = beta1 * m + (1 - beta1) * grad\n",
    "        # Update biased second moment\n",
    "        v = beta2 * v + (1 - beta2) * grad**2\n",
    "        \n",
    "        # Bias correction\n",
    "        m_hat = m / (1 - beta1**t)\n",
    "        v_hat = v / (1 - beta2**t)\n",
    "        \n",
    "        # Update\n",
    "        x = x - learning_rate * m_hat / (np.sqrt(v_hat) + eps)\n",
    "        \n",
    "        x_history.append(x.copy())\n",
    "        f_history.append(f(x))\n",
    "    \n",
    "    return np.array(x_history), np.array(f_history)\n",
    "\n",
    "# Test Adam\n",
    "x_hist_adam, f_hist_adam = adam(f, grad_f, x0, learning_rate=0.5, n_iters=20)\n",
    "\n",
    "plt.plot(f_hist, label='GD')\n",
    "plt.plot(f_hist_mom, label='Momentum')\n",
    "plt.plot(f_hist_adam, label='Adam')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('f(x)')\n",
    "plt.legend()\n",
    "plt.title('Optimizer Comparison')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "1. Derive the gradient of logistic regression loss with L2 regularization\n",
    "2. Implement gradient descent for logistic regression\n",
    "3. Compare convergence of GD, Momentum, Adam on Rosenbrock function\n",
    "4. Visualize the effect of learning rate on convergence"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
