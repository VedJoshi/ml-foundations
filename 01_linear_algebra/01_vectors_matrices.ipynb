{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Algebra Foundations\n",
    "\n",
    "## Key Concepts\n",
    "- Vectors, matrices, linear maps\n",
    "- Eigenvalues/eigenvectors\n",
    "- SVD (Singular Value Decomposition)\n",
    "- PCA (Principal Component Analysis)\n",
    "\n",
    "## References\n",
    "- Bishop PRML: Chapter 1.2 (Probability distributions involve linear algebra)\n",
    "- Matrix Cookbook: Sections 1-5\n",
    "- MML Book: Chapters 2-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Matrix-Vector Multiplication with Visualization\n",
    "\n",
    "Given matrix $A \\in \\mathbb{R}^{m \\times n}$ and vector $x \\in \\mathbb{R}^n$:\n",
    "\n",
    "$$y = Ax \\quad \\text{where} \\quad y_i = \\sum_{j=1}^{n} A_{ij} x_j$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement matrix-vector multiplication from scratch\n",
    "def matrix_vector_mult(A, x):\n",
    "    \"\"\"Matrix-vector multiplication without using np.dot\"\"\"\n",
    "    m, n = A.shape\n",
    "    assert x.shape[0] == n, 'Dimensions must match'\n",
    "    \n",
    "    y = np.zeros(m)\n",
    "    for i in range(m):\n",
    "        for j in range(n):\n",
    "            y[i] += A[i, j] * x[j]\n",
    "    return y\n",
    "\n",
    "# Test\n",
    "A = np.array([[1, 2], [3, 4], [5, 6]])\n",
    "x = np.array([1, 2])\n",
    "print('Manual:', matrix_vector_mult(A, x))\n",
    "print('NumPy:', A @ x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_matrix_vector_mult():",
    "    \"\"\"Visualize matrix-vector multiplication as linear transformation\"\"\"\n",
    "    # Create figure\n",
    "    fig = plt.figure(figsize=(12, 8))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    \n",
    "    # Original basis vectors (2D for compatibility with 2x2 matrix)\n",
    "    origin = np.array([0, 0, 0])\n",
    "    e1 = np.array([1, 0])  # 2D vector\n",
    "    e2 = np.array([0, 1])  # 2D vector\n",
    "    \n",
    "    # Transformation matrix (2D in 3D space)\n",
    "    A = np.array([[2, 1], [-1, 2]])\n",
    "    \n",
    "    # Transformed basis vectors (now works: 2x2 @ 2x1 = 2x1)\n",
    "    Ae1 = np.append(A @ e1, 0)\n",
    "    Ae2 = np.append(A @ e2, 0)\n",
    "    \n",
    "    # Convert original vectors to 3D for plotting\n",
    "    e1_3d = np.append(e1, 0)\n",
    "    e2_3d = np.append(e2, 0)\n",
    "    \n",
    "    # Plot original basis\n",
    "    ax.quiver(*origin, *e1_3d, color='blue', length=1.0, arrow_length_ratio=0.1, label='Original e1')\n",
    "    ax.quiver(*origin, *e2_3d, color='green', length=1.0, arrow_length_ratio=0.1, label='Original e2')\n",
    "    \n",
    "    # Plot transformed basis\n",
    "    ax.quiver(*origin, *Ae1, color='red', length=1.0, arrow_length_ratio=0.1, label='Transformed e1')\n",
    "    ax.quiver(*origin, *Ae2, color='orange', length=1.0, arrow_length_ratio=0.1, label='Transformed e2')\n",
    "    \n",
    "    # Plot a sample vector and its transformation\n",
    "    x = np.array([1, 0.5])\n",
    "    x_3d = np.append(x, 0)\n",
    "    Ax_3d = np.append(A @ x, 0)\n",
    "    \n",
    "    ax.quiver(*origin, *x_3d, color='purple', length=1.0, arrow_length_ratio=0.1, label='Original x', linestyle='--')\n",
    "    ax.quiver(*origin, *Ax_3d, color='brown', length=1.0, arrow_length_ratio=0.1, label='Transformed Ax', linestyle='--')\n",
    "    \n",
    "    ax.set_xlim([-3, 3])\n",
    "    ax.set_ylim([-3, 3])\n",
    "    ax.set_zlim([-1, 1])\n",
    "    ax.set_xlabel('X')\n",
    "    ax.set_ylabel('Y')\n",
    "    ax.set_zlabel('Z')\n",
    "    ax.set_title('Matrix-Vector Multiplication as Linear Transformation')\n",
    "    ax.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print('Transformation matrix A:')\n",
    "    print(A)\n",
    "    print(f'Original vector x: {x}')\n",
    "    print(f'Transformed vector Ax: {A @ x}')\n",
    "    print('\\nGeometric interpretation:')\n",
    "    print('- The matrix A transforms the basis vectors')\n",
    "    print('- Any vector x is transformed by applying A to its components')\n",
    "    print('- This shows how linear transformations work geometrically')\n",
    "\n",
    "plot_matrix_vector_mult()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Worked Example: Matrix-Vector Multiplication\n",
    "\n",
    "**Problem:** Compute y = Ax where:\n",
    "```\n",
    "A = [[1, 2, 3],\n",
    "     [4, 5, 6]]\n",
    "x = [7, 8, 9]\n",
    "```\n",
    "\n",
    "**Step-by-step solution:**\n",
    "\n",
    "1. **Row view (dot product):**\n",
    "   ```\n",
    "   y₁ = (1×7) + (2×8) + (3×9) = 7 + 16 + 27 = 50\n",
    "   y₂ = (4×7) + (5×8) + (6×9) = 28 + 40 + 54 = 122\n",
    "   ```\n",
    "\n",
    "2. **Column view (linear combination):**\n",
    "   ```\n",
    "   y = 7×[1,4] + 8×[2,5] + 9×[3,6]\n",
    "     = [7,28] + [16,40] + [27,54]\n",
    "     = [50, 122]\n",
    "   ```\n",
    "\n",
    "3. **Geometric interpretation:**\n",
    "   - The result is a linear combination of A's columns\n",
    "   - x contains the weights for this combination\n",
    "   - y lies in the column space of A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ML Application: Feature Transformation\n",
    "\n",
    "In machine learning, matrix-vector multiplication is used for:\n",
    "\n",
    "1. **Linear regression:** ŷ = Xw (design matrix × weight vector)\n",
    "2. **Neural networks:** z = Wx + b (weight matrix × input vector)\n",
    "3. **PCA projection:** Z = XV (data × principal components)\n",
    "\n",
    "**Key insight:** Understanding matrix-vector multiplication helps you understand how models transform input features into predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Eigenvalue Decomposition with Visualization\n",
    "\n",
    "For a square matrix $A$, eigenvalues $\\lambda$ and eigenvectors $v$ satisfy:\n",
    "\n",
    "$$Av = \\lambda v$$\n",
    "\n",
    "Decomposition: $A = V \\Lambda V^{-1}$ where $\\Lambda$ is diagonal matrix of eigenvalues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_eigenvectors():",
    "    \"\"\"Visualize eigenvectors and eigenvalues\"\"\"\n",
    "    # Create a symmetric matrix (guarantees real eigenvalues)\n",
    "    A = np.array([[2, 1], [1, 2]])\n",
    "    eigenvalues, eigenvectors = np.linalg.eig(A)\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    \n",
    "    # Plot the transformation\n",
    "    ax1 = axes[0]\n",
    "    \n",
    "    # Create a circle of points\n",
    "    theta = np.linspace(0, 2*np.pi, 100)\n",
    "    circle = np.column_stack([np.cos(theta), np.sin(theta)])\n",
    "    transformed = circle @ A\n",
    "    \n",
    "    # Plot original circle\n",
    "    ax1.plot(circle[:, 0], circle[:, 1], 'b-', alpha=0.5, label='Original circle')\n",
    "    ax1.quiver(0, 0, circle[0, 0], circle[0, 1], color='blue', scale=20, label='Original vector')\n",
    "    \n",
    "    # Plot transformed circle (ellipse)\n",
    "    ax1.plot(transformed[:, 0], transformed[:, 1], 'r-', alpha=0.5, label='Transformed circle')\n",
    "    ax1.quiver(0, 0, transformed[0, 0], transformed[0, 1], color='red', scale=20, label='Transformed vector')\n",
    "    \n",
    "    # Plot eigenvectors\n",
    "    for i, (lam, v) in enumerate(zip(eigenvalues, eigenvectors.T)):\n",
    "        ax1.quiver(0, 0, v[0], v[1], color='green', scale=15,\n",
    "                  label=f'Eigenvector {i+1} ($\\lambda$={lam:.2f})',\n",
    "                  linestyle='--', linewidth=2)\n",
    "    ax1.set_xlim([-3, 3])\n",
    "    ax1.set_ylim([-3, 3])\n",
    "    ax1.set_aspect('equal')\n",
    "    ax1.set_title('Linear Transformation and Eigenvectors')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True)\n",
    "    \n",
    "    # Plot eigenvalues\n",
    "    ax2 = axes[1]\n",
    "    bars = ax2.bar(range(len(eigenvalues)), eigenvalues, color=['green', 'orange'])\n",
    "    ax2.set_xlabel('Eigenvector index')\n",
    "    ax2.set_ylabel('Eigenvalue')\n",
    "    ax2.set_title('Eigenvalues (show scaling factors)')\n",
    "    ax2.set_xticks(range(len(eigenvalues)))\n",
    "    ax2.set_xticklabels([f'v{i+1}' for i in range(len(eigenvalues))])\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax2.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{height:.2f}',\n",
    "                ha='center', va='bottom')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f'Matrix A:\\n{A}')\n",
    "    print(f'\\nEigenvalues: {eigenvalues}')\n",
    "    print(f'Eigenvectors:\\n{eigenvectors}')\n",
    "    print(f'\\nGeometric interpretation:')\n",
    "    print(f'- Eigenvectors are directions that only get scaled (not rotated)')\n",
    "    print(f'- Eigenvalues indicate how much scaling occurs along each eigenvector')\n",
    "    print(f'- The circle becomes an ellipse stretched along eigenvector directions')\n",
    "\n",
    "plot_eigenvectors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eigenvalue decomposition example\n",
    "A = np.array([[4, 2], [1, 3]])\n",
    "eigenvalues, eigenvectors = np.linalg.eig(A)\n",
    "\n",
    "print('Eigenvalues:', eigenvalues)\n",
    "print('Eigenvectors:\\n', eigenvectors)\n",
    "\n",
    "# Verify: A @ v = lambda * v\n",
    "for i in range(len(eigenvalues)):\n",
    "    v = eigenvectors[:, i]\n",
    "    lam = eigenvalues[i]\n",
    "    print(f'\\nVerify λ_{i}: A@v = {A @ v}, λ*v = {lam * v}')\n",
    "    print(f'Match: {np.allclose(A @ v, lam * v)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Worked Example: Finding Eigenvalues\n",
    "\n",
    "**Problem:** Find eigenvalues of matrix:\n",
    "```\n",
    "A = [[2, 1],\n",
    "     [1, 2]]\n",
    "```\n",
    "\n",
    "**Step-by-step solution:**\n",
    "\n",
    "1. **Characteristic equation:** det(A - λI) = 0\n",
    "   ```\n",
    "   |2-λ   1  |\n",
    "   |1    2-λ| = (2-λ)² - 1 = λ² - 4λ + 3 = 0\n",
    "   ```\n",
    "\n",
    "2. **Solve quadratic equation:**\n",
    "   ```\n",
    "   λ = [4 ± √(16 - 12)] / 2 = [4 ± 2]/2\n",
    "   λ₁ = 3, λ₂ = 1\n",
    "   ```\n",
    "\n",
    "3. **Find eigenvectors:**\n",
    "   - For λ=3: (A-3I)v=0 → [-1,1;1,-1]v=0 → v₁=[1,1]\n",
    "   - For λ=1: (A-I)v=0 → [1,1;1,1]v=0 → v₂=[1,-1]\n",
    "\n",
    "**Verification:**\n",
    "- A @ [1,1] = [3,3] = 3*[1,1] ✓\n",
    "- A @ [1,-1] = [1,-1] = 1*[1,-1] ✓"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ML Application: Principal Component Analysis (PCA)\n",
    "\n",
    "Eigendecomposition is used in PCA:\n",
    "\n",
    "1. **Covariance matrix:** C = (1/n)XᵀX\n",
    "2. **Eigendecomposition:** C = VΛVᵀ\n",
    "3. **Principal components:** Columns of V (eigenvectors)\n",
    "4. **Variance explained:** Eigenvalues Λ\n",
    "\n",
    "**Key insight:** Eigenvectors point in directions of maximum variance in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. SVD (Singular Value Decomposition) with Visualization\n",
    "\n",
    "For any matrix $A \\in \\mathbb{R}^{m \\times n}$:\n",
    "\n",
    "$$A = U \\Sigma V^T$$\n",
    "\n",
    "Where:\n",
    "- $U \\in \\mathbb{R}^{m \\times m}$ (left singular vectors)\n",
    "- $\\Sigma \\in \\mathbb{R}^{m \\times n}$ (diagonal matrix of singular values)\n",
    "- $V \\in \\mathbb{R}^{n \\times n}$ (right singular vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_svd_decomposition():",
    "    \"\"\"Visualize SVD decomposition\"\"\"\n",
    "    # Create a rank-2 matrix\n",
    "    A = np.array([[3, 1, 1],\n",
    "                  [1, 3, 1],\n",
    "                  [1, 1, 3]])\n",
    "    \n",
    "    U, S, Vt = np.linalg.svd(A)\n",
    "    V = Vt.T\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    \n",
    "    # Plot original matrix\n",
    "    im1 = axes[0].imshow(A, cmap='viridis')\n",
    "    axes[0].set_title('Original Matrix A')\n",
    "    fig.colorbar(im1, ax=axes[0])\n",
    "    \n",
    "    # Plot U matrix\n",
    "    im2 = axes[1].imshow(U, cmap='viridis')\n",
    "    axes[1].set_title('U (Left Singular Vectors)')\n",
    "    fig.colorbar(im2, ax=axes[1])\n",
    "    \n",
    "    # Plot V matrix\n",
    "    im3 = axes[2].imshow(V, cmap='viridis')\n",
    "    axes[2].set_title('V (Right Singular Vectors)')\n",
    "    fig.colorbar(im3, ax=axes[2])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f'Singular values: {S}')\n",
    "    print(f'Condition number: {S[0]/S[-1]:.2f}')\n",
    "    print(f'\\nSVD interpretation:')\n",
    "    print(f'- U contains output space basis (column space of A)')\n",
    "    print(f'- Σ contains scaling factors (importance of each basis)')\n",
    "    print(f'- V contains input space basis (row space of A)')\n",
    "    print(f'- Condition number indicates numerical stability')\n",
    "\n",
    "plot_svd_decomposition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVD example\n",
    "A = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "U, S, Vt = np.linalg.svd(A, full_matrices=False)\n",
    "\n",
    "print('U shape:', U.shape)\n",
    "print('S (singular values):', S)\n",
    "print('V^T shape:', Vt.shape)\n",
    "\n",
    "# Reconstruct A\n",
    "A_reconstructed = U @ np.diag(S) @ Vt\n",
    "print('\\nReconstruction error:', np.linalg.norm(A - A_reconstructed))\n",
    "\n",
    "# Show relationship to eigendecomposition\n",
    "print('\\nRelationship to eigenvalues:')\n",
    "print(f'A@A.T eigenvalues: {np.linalg.eig(A @ A.T)[0]}')\n",
    "print(f'S^2: {S**2}')\n",
    "print(f'Match: {np.allclose(np.linalg.eig(A @ A.T)[0], S**2)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Worked Example: SVD Computation\n",
    "\n",
    "**Problem:** Compute SVD of:\n",
    "```\n",
    "A = [[1, 2],\n",
    "     [3, 4],\n",
    "     [5, 6]]\n",
    "```\n",
    "\n",
    "**Step-by-step solution:**\n",
    "\n",
    "1. **Compute AᵀA:**\n",
    "   ```\n",
    "   AᵀA = [[1,3,5], [2,4,6]] @ [[1,2], [3,4], [5,6]]\n",
    "       = [[35, 44], [44, 56]]\n",
    "   ```\n",
    "\n",
    "2. **Find eigenvalues of AᵀA:**\n",
    "   ```\n",
    "   det([[35-λ, 44], [44, 56-λ]]) = 0\n",
    "   λ² - 91λ + 1254 = 0\n",
    "   λ = [91 ± √(8281 - 5016)]/2\n",
    "   λ = [91 ± √3265]/2\n",
    "   σ₁² ≈ 84.87, σ₂² ≈ 6.13\n",
    "   σ₁ ≈ 9.21, σ₂ ≈ 2.48\n",
    "   ```\n",
    "\n",
    "3. **Singular values:** σ₁ ≈ 9.21, σ₂ ≈ 2.48\n",
    "\n",
    "**Verification:**\n",
    "- U, Σ, Vᵀ = np.linalg.svd(A)\n",
    "- A ≈ UΣVᵀ (reconstruction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ML Application: Dimensionality Reduction and Data Compression\n",
    "\n",
    "SVD is used in ML for:\n",
    "\n",
    "1. **PCA:** X = UΣVᵀ → principal components in V\n",
    "2. **Data compression:** Keep only top-k singular values\n",
    "3. **Latent semantic analysis:** Document-term matrix decomposition\n",
    "4. **Recommender systems:** User-item matrix factorization\n",
    "\n",
    "**Key insight:** SVD reveals the intrinsic dimensionality of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. PCA via SVD with Visualization\n",
    "\n",
    "PCA finds directions of maximum variance. For centered data $X$:\n",
    "\n",
    "1. Compute SVD: $X = U \\Sigma V^T$\n",
    "2. Principal components are columns of $V$\n",
    "3. Projected data: $Z = XV = U\\Sigma$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pca_visualization():",
    "    \"\"\"Visualize PCA on 2D data\"\"\"\n",
    "    # Generate correlated data\n",
    "    np.random.seed(42)\n",
    "    X = np.random.randn(100, 2)\n",
    "    X = X @ [[2, 1], [1, 1]]  # Make it correlated\n",
    "    # Center the data\n",
    "    X_centered = X - np.mean(X, axis=0)\n",
    "    \n",
    "    # Perform SVD\n",
    "    U, S, Vt = np.linalg.svd(X_centered, full_matrices=False)\n",
    "    V = Vt.T\n",
    "    \n",
    "    # Project onto principal components\n",
    "    Z = X_centered @ V\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    \n",
    "    # Original data\n",
    "    axes[0].scatter(X[:, 0], X[:, 1], alpha=0.6)\n",
    "    axes[0].set_title('Original Data')\n",
    "    axes[0].set_aspect('equal')\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot principal components\n",
    "    for i, v in enumerate(V.T):\n",
    "        axes[0].quiver(np.mean(X[:, 0]), np.mean(X[:, 1]),\n",
    "                       v[0]*S[i], v[1]*S[i],\n",
    "                       color=['red', 'green'][i],\n",
    "                       scale=20, width=0.01,\n",
    "                       label=f'PC{i+1} (var={S[i]**2/np.sum(S**2):.2%})')\n",
    "    axes[0].legend()\n",
    "    \n",
    "    # Projected data\n",
    "    axes[1].scatter(Z[:, 0], Z[:, 1], alpha=0.6)\n",
    "    axes[1].set_title('PCA Projected Data')\n",
    "    axes[1].set_aspect('equal')\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f'Explained variance ratio: {S**2 / np.sum(S**2)}')\n",
    "    print(f'Total variance explained: {np.sum(S**2):.2f}')\n",
    "    print(f'\\nPCA interpretation:')\n",
    "    print(f'- PC1 captures the direction of maximum variance')\n",
    "    print(f'- PC2 captures the direction of next maximum variance')\n",
    "    print(f'- The projection preserves the essential structure of the data')\n",
    "\n",
    "plot_pca_visualization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement PCA from scratch using SVD\n",
    "def pca_svd(X, n_components):\n",
    "    \"\"\"PCA via SVD\n",
    "    \n",
    "    Args:\n",
    "        X: Data matrix (n_samples, n_features)\n",
    "        n_components: Number of principal components\n",
    "    \n",
    "    Returns:\n",
    "        Z: Projected data (n_samples, n_components)\n",
    "        V: Principal component directions (n_features, n_components)\n",
    "        explained_variance_ratio: Variance explained by each component\n",
    "    \"\"\"\n",
    "    # Center the data\n",
    "    X_centered = X - np.mean(X, axis=0)\n",
    "    \n",
    "    # SVD\n",
    "    U, S, Vt = np.linalg.svd(X_centered, full_matrices=False)\n",
    "    \n",
    "    # Principal components\n",
    "    V = Vt.T[:, :n_components]\n",
    "    \n",
    "    # Project data\n",
    "    Z = X_centered @ V\n",
    "    \n",
    "    # Explained variance\n",
    "    total_var = np.sum(S**2)\n",
    "    explained_variance_ratio = (S[:n_components]**2) / total_var\n",
    "    \n",
    "    return Z, V, explained_variance_ratio\n",
    "\n",
    "# Test with random data\n",
    "np.random.seed(42)\n",
    "X = np.random.randn(100, 5)\n",
    "Z, V, var_ratio = pca_svd(X, n_components=2)\n",
    "print('Projected shape:', Z.shape)\n",
    "print('Explained variance ratio:', var_ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Worked Example: PCA Step-by-Step\n",
    "\n",
    "**Problem:** Perform PCA on this 3-point dataset:\n",
    "```\n",
    "X = [[1, 2],\n",
    "     [2, 3],\n",
    "     [3, 4]]\n",
    "```\n",
    "\n",
    "**Step-by-step solution:**\n",
    "\n",
    "1. **Center the data:**\n",
    "   ```\n",
    "   mean = [2, 3]\n",
    "   X_centered = [[-1, -1], [0, 0], [1, 1]]\n",
    "   ```\n",
    "\n",
    "2. **Compute covariance matrix:**\n",
    "   ```\n",
    "   C = (1/3) X_centeredᵀ X_centered\n",
    "     = (1/3) [[-1,0,1], [-1,0,1]] @ [[-1,0,1], [-1,0,1]]ᵀ\n",
    "     = (1/3) [[2, 2], [2, 2]]\n",
    "     = [[0.666, 0.666], [0.666, 0.666]]\n",
    "   ```\n",
    "\n",
    "3. **Eigendecomposition:**\n",
    "   ```\n",
    "   Eigenvalues: λ₁ ≈ 1.333, λ₂ = 0\n",
    "   Eigenvectors: v₁ = [0.707, 0.707], v₂ = [-0.707, 0.707]\n",
    "   ```\n",
    "\n",
    "4. **Project data:**\n",
    "   ```\n",
    "   Z = X_centered @ [v₁]\n",
    "     = [[-1.414], [0], [1.414]]\n",
    "   ```\n",
    "\n",
    "**Interpretation:**\n",
    "- All points lie on a line (rank 1 data)\n",
    "- PC1 captures 100% of variance\n",
    "- PC2 captures 0% variance (noise direction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ML Application: Feature Extraction and Visualization\n",
    "\n",
    "PCA is used in ML for:\n",
    "\n",
    "1. **Dimensionality reduction:** Reduce feature space while preserving variance\n",
    "2. **Visualization:** Project high-dimensional data to 2D/3D\n",
    "3. **Noise reduction:** Discard low-variance components\n",
    "4. **Feature engineering:** Create uncorrelated features\n",
    "\n",
    "**Key insight:** PCA helps visualize and understand the intrinsic structure of high-dimensional data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "1. Implement matrix-matrix multiplication from scratch\n",
    "2. Derive why $A^T A$ and $A A^T$ have the same non-zero eigenvalues\n",
    "3. Implement PCA via eigendecomposition of covariance matrix\n",
    "4. Apply PCA to MNIST and visualize first 2 components\n",
    "5. Create an interactive visualization showing how SVD captures data structure\n",
    "6. Explain geometrically why eigenvectors are important for understanding linear transformations"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
