{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probability & Statistics for ML\n",
    "\n",
    "## Key Concepts\n",
    "- Random variables, expectations, variance\n",
    "- Common distributions (Gaussian, Bernoulli, Categorical)\n",
    "- Bayes' rule\n",
    "- Maximum Likelihood Estimation (MLE)\n",
    "\n",
    "## References\n",
    "- Bishop PRML: Chapters 1-2\n",
    "- FOML: Probability sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Gaussian Distribution\n",
    "\n",
    "PDF: $p(x|\\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right)$\n",
    "\n",
    "Log-likelihood: $\\log p(x|\\mu, \\sigma^2) = -\\frac{1}{2}\\log(2\\pi\\sigma^2) - \\frac{(x-\\mu)^2}{2\\sigma^2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Gaussian distributions\n",
    "x = np.linspace(-5, 5, 100)\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "for mu, sigma in [(0, 1), (0, 0.5), (1, 1.5)]:\n",
    "    y = stats.norm.pdf(x, mu, sigma)\n",
    "    plt.plot(x, y, label=f'μ={mu}, σ={sigma}')\n",
    "\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('p(x)')\n",
    "plt.title('Gaussian Distributions')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Maximum Likelihood Estimation (MLE)\n",
    "\n",
    "For Gaussian with i.i.d. samples $\\{x_1, ..., x_n\\}$:\n",
    "\n",
    "$$\\hat{\\mu}_{MLE} = \\frac{1}{n}\\sum_{i=1}^n x_i$$\n",
    "\n",
    "$$\\hat{\\sigma}^2_{MLE} = \\frac{1}{n}\\sum_{i=1}^n (x_i - \\hat{\\mu})^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_mle(X):\n",
    "    \"\"\"MLE for Gaussian distribution\"\"\"\n",
    "    mu_mle = np.mean(X)\n",
    "    sigma2_mle = np.mean((X - mu_mle)**2)\n",
    "    return mu_mle, sigma2_mle\n",
    "\n",
    "# Generate samples and estimate\n",
    "np.random.seed(42)\n",
    "true_mu, true_sigma = 2.0, 1.5\n",
    "X = np.random.normal(true_mu, true_sigma, size=1000)\n",
    "\n",
    "mu_mle, sigma2_mle = gaussian_mle(X)\n",
    "print(f\"True: μ={true_mu}, σ²={true_sigma**2}\")\n",
    "print(f\"MLE:  μ={mu_mle:.3f}, σ²={sigma2_mle:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Bayes' Rule\n",
    "\n",
    "$$P(\\theta|D) = \\frac{P(D|\\theta)P(\\theta)}{P(D)}$$\n",
    "\n",
    "- $P(\\theta|D)$: Posterior (what we want)\n",
    "- $P(D|\\theta)$: Likelihood\n",
    "- $P(\\theta)$: Prior\n",
    "- $P(D)$: Evidence (normalizing constant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bayesian coin flip example\n",
    "# Prior: Beta(a, b) on probability p of heads\n",
    "# Likelihood: Binomial\n",
    "# Posterior: Beta(a + heads, b + tails)\n",
    "\n",
    "def bayesian_coin_update(prior_a, prior_b, n_heads, n_tails):\n",
    "    \"\"\"Update Beta prior with observed coin flips\"\"\"\n",
    "    post_a = prior_a + n_heads\n",
    "    post_b = prior_b + n_tails\n",
    "    return post_a, post_b\n",
    "\n",
    "# Start with uniform prior Beta(1, 1)\n",
    "prior_a, prior_b = 1, 1\n",
    "\n",
    "# Observe 7 heads, 3 tails\n",
    "n_heads, n_tails = 7, 3\n",
    "post_a, post_b = bayesian_coin_update(prior_a, prior_b, n_heads, n_tails)\n",
    "\n",
    "p = np.linspace(0, 1, 100)\n",
    "prior_pdf = stats.beta.pdf(p, prior_a, prior_b)\n",
    "posterior_pdf = stats.beta.pdf(p, post_a, post_b)\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(p, prior_pdf, label='Prior Beta(1,1)')\n",
    "plt.plot(p, posterior_pdf, label=f'Posterior Beta({post_a},{post_b})')\n",
    "plt.axvline(n_heads/(n_heads+n_tails), color='r', linestyle='--', label='MLE')\n",
    "plt.xlabel('p (probability of heads)')\n",
    "plt.ylabel('Density')\n",
    "plt.title('Bayesian Update for Coin Flip')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Monte Carlo Estimation\n",
    "\n",
    "Estimate expectations via sampling:\n",
    "$$E[f(X)] \\approx \\frac{1}{n}\\sum_{i=1}^n f(x_i) \\quad \\text{where } x_i \\sim p(x)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def monte_carlo_expectation(f, samples):\n",
    "    \"\"\"Estimate E[f(X)] via Monte Carlo\"\"\"\n",
    "    return np.mean(f(samples))\n",
    "\n",
    "# Estimate E[X^2] for X ~ N(0, 1) (should be 1)\n",
    "X = np.random.normal(0, 1, size=10000)\n",
    "f = lambda x: x**2\n",
    "\n",
    "estimate = monte_carlo_expectation(f, X)\n",
    "print(f\"Monte Carlo estimate of E[X²]: {estimate:.4f}\")\n",
    "print(f\"True value: 1.0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "1. Derive the MLE for Bernoulli distribution\n",
    "2. Implement MLE for multivariate Gaussian\n",
    "3. Use Monte Carlo to estimate π (hint: unit circle in unit square)\n",
    "4. Implement Bayesian linear regression with conjugate prior"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
