{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaled Dot-Product Attention\n",
    "\n",
    "Derive and implement the core attention mechanism from *Attention Is All You Need* (Vaswani et al., 2017).\n",
    "\n",
    "**Objective:** Full derivation of scaled dot-product attention, including the scaling factor, masking, and numerical stability. Implement from scratch in PyTorch and verify against the built-in.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mathematical Foundation: Attention as Soft Lookup\n",
    "\n",
    "Attention is a differentiable dictionary. Given a **query**, compute similarity to a set of **keys**, then use the resulting weights to take a weighted sum over **values**.\n",
    "\n",
    "**Definitions:**\n",
    "\n",
    "$$Q \\in \\mathbb{R}^{n \\times d_k} \\quad \\text{(queries — } n \\text{ query vectors, each of dimension } d_k\\text{)}$$\n",
    "$$K \\in \\mathbb{R}^{m \\times d_k} \\quad \\text{(keys — } m \\text{ key vectors, each of dimension } d_k\\text{)}$$\n",
    "$$V \\in \\mathbb{R}^{m \\times d_v} \\quad \\text{(values — } m \\text{ value vectors, each of dimension } d_v\\text{)}$$\n",
    "\n",
    "**Step 1 — Raw attention scores:**\n",
    "\n",
    "$$S = QK^T \\in \\mathbb{R}^{n \\times m}$$\n",
    "\n",
    "Entry $S_{ij} = q_i^T k_j$ measures similarity between query $i$ and key $j$. This is a dot-product similarity — higher means more aligned.\n",
    "\n",
    "**Step 2 — Attention weights:**\n",
    "\n",
    "$$A = \\text{softmax}(S, \\text{dim}=-1) \\in \\mathbb{R}^{n \\times m}$$\n",
    "\n",
    "Softmax is applied row-wise: $A_{ij} = \\frac{\\exp(S_{ij})}{\\sum_{l=1}^{m} \\exp(S_{il})}$. Each row of $A$ is a probability distribution over the $m$ keys.\n",
    "\n",
    "**Step 3 — Output:**\n",
    "\n",
    "$$O = AV \\in \\mathbb{R}^{n \\times d_v}$$\n",
    "\n",
    "Row $i$ of the output is $o_i = \\sum_{j=1}^{m} A_{ij} v_j$ — a weighted average of value vectors, where the weights come from query-key similarity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Scaling Factor $\\sqrt{d_k}$\n",
    "\n",
    "Assume query and key components are i.i.d. with zero mean and unit variance:\n",
    "\n",
    "$$q_i, k_j \\sim \\text{distribution with } E[q_i] = 0, \\; \\text{Var}(q_i) = 1 \\quad \\text{for } i = 1, \\ldots, d_k$$\n",
    "\n",
    "The dot product between a single query and key vector:\n",
    "\n",
    "$$q \\cdot k = \\sum_{i=1}^{d_k} q_i k_i$$\n",
    "\n",
    "**Mean:**\n",
    "\n",
    "$$E[q \\cdot k] = \\sum_{i=1}^{d_k} E[q_i k_i] = \\sum_{i=1}^{d_k} E[q_i] E[k_i] = 0$$\n",
    "\n",
    "where the second equality uses independence of $q_i$ and $k_i$.\n",
    "\n",
    "**Variance:**\n",
    "\n",
    "$$\\text{Var}(q \\cdot k) = \\text{Var}\\left(\\sum_{i=1}^{d_k} q_i k_i\\right) = \\sum_{i=1}^{d_k} \\text{Var}(q_i k_i)$$\n",
    "\n",
    "where the last step uses independence across components. For each term:\n",
    "\n",
    "$$\\text{Var}(q_i k_i) = E[q_i^2 k_i^2] - (E[q_i k_i])^2 = E[q_i^2] E[k_i^2] - 0 = 1 \\cdot 1 = 1$$\n",
    "\n",
    "Therefore:\n",
    "\n",
    "$$\\text{Var}(q \\cdot k) = d_k$$\n",
    "\n",
    "**The problem:** As $d_k$ grows, dot products have standard deviation $\\sqrt{d_k}$. Large-magnitude inputs to softmax push it into saturation regions where $\\frac{\\partial \\text{softmax}}{\\partial x} \\approx 0$ — gradients vanish.\n",
    "\n",
    "Concretely, for $d_k = 512$, dot products have std $\\approx 22.6$. A softmax input of $[0, 0, \\ldots, 22.6, \\ldots, 0]$ produces a near-one-hot output.\n",
    "\n",
    "**The fix:** Divide by $\\sqrt{d_k}$:\n",
    "\n",
    "$$\\text{Var}\\left(\\frac{q \\cdot k}{\\sqrt{d_k}}\\right) = \\frac{1}{d_k} \\text{Var}(q \\cdot k) = \\frac{d_k}{d_k} = 1$$\n",
    "\n",
    "**Final formula:**\n",
    "\n",
    "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right) V$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Masking\n",
    "\n",
    "Two common mask types, both applied **before** softmax:\n",
    "\n",
    "**Padding mask:** Variable-length sequences are padded to the same length. Padding tokens should not receive attention. Set their scores to $-\\infty$ (in practice, a large negative like $-10^9$) so $\\text{softmax}(-\\infty) \\approx 0$.\n",
    "\n",
    "$$S_{ij}^{\\text{masked}} = \\begin{cases} S_{ij} & \\text{if position } j \\text{ is real} \\\\ -\\infty & \\text{if position } j \\text{ is padding} \\end{cases}$$\n",
    "\n",
    "**Causal (look-ahead) mask:** In autoregressive decoding, position $i$ must not attend to positions $j > i$. This is an upper-triangular mask:\n",
    "\n",
    "$$S_{ij}^{\\text{masked}} = \\begin{cases} S_{ij} & \\text{if } j \\leq i \\\\ -\\infty & \\text{if } j > i \\end{cases}$$\n",
    "\n",
    "Implementation: construct a binary mask $M$ (1 = attend, 0 = ignore), then compute $S + (1 - M) \\cdot (-10^9)$, or equivalently set positions where $M = 0$ to $-10^9$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numerical Stability of Softmax\n",
    "\n",
    "Naive computation:\n",
    "\n",
    "$$\\text{softmax}(x_i) = \\frac{\\exp(x_i)}{\\sum_j \\exp(x_j)}$$\n",
    "\n",
    "If $x_i$ is large (e.g., 1000), $\\exp(x_i)$ overflows to `inf`. If $x_i$ is very negative, $\\exp(x_i)$ underflows to 0, and the denominator can become 0.\n",
    "\n",
    "**Stable version:** subtract the maximum before exponentiation:\n",
    "\n",
    "$$\\text{softmax}(x_i) = \\frac{\\exp(x_i - \\max_j x_j)}{\\sum_j \\exp(x_j - \\max_j x_j)}$$\n",
    "\n",
    "This is mathematically identical (multiply numerator and denominator by $\\exp(-\\max_j x_j)$) but now the largest exponent is $\\exp(0) = 1$, preventing overflow. The denominator is always $\\geq 1$.\n",
    "\n",
    "PyTorch's `F.softmax` and `torch.softmax` handle this internally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "\n",
    "Implement `scaled_dot_product_attention` following the derivation above.\n",
    "\n",
    "**Specification:**\n",
    "- **Input:** `Q` (batch, n, d_k), `K` (batch, m, d_k), `V` (batch, m, d_v), `mask` (optional)\n",
    "- **Output:** `(output, attention_weights)` with shapes `(batch, n, d_v)` and `(batch, n, m)`\n",
    "- **Steps:**\n",
    "  1. Compute raw scores: `Q @ K^T` $\\to$ `(batch, n, m)`\n",
    "  2. Scale by $1/\\sqrt{d_k}$\n",
    "  3. Apply mask: where `mask == 0`, set score to `-1e9`\n",
    "  4. Softmax along last dimension\n",
    "  5. Multiply by `V`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(Q, K, V, mask=None):\n",
    "    \"\"\"\n",
    "    Scaled dot-product attention.\n",
    "\n",
    "    Args:\n",
    "        Q: (batch, n, d_k) query matrix\n",
    "        K: (batch, m, d_k) key matrix\n",
    "        V: (batch, m, d_v) value matrix\n",
    "        mask: (batch, 1, m) or (batch, n, m) — 1 = attend, 0 = ignore\n",
    "\n",
    "    Returns:\n",
    "        output: (batch, n, d_v) weighted sum of values\n",
    "        attention_weights: (batch, n, m) attention distribution\n",
    "    \"\"\"\n",
    "    d_k = Q.size(-1)\n",
    "\n",
    "    # TODO: Compute attention scores (Q @ K^T)\n",
    "    # scores shape: (batch, n, m)\n",
    "    scores = ...\n",
    "\n",
    "    # TODO: Scale by sqrt(d_k)\n",
    "    scores = ...\n",
    "\n",
    "    # TODO: Apply mask (set masked positions to -1e9)\n",
    "    if mask is not None:\n",
    "        scores = ...\n",
    "\n",
    "    # TODO: Apply softmax along last dimension\n",
    "    attention_weights = ...\n",
    "\n",
    "    # TODO: Compute output (attention_weights @ V)\n",
    "    output = ...\n",
    "\n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify implementation against PyTorch's built-in\n",
    "batch, n, m, d_k, d_v = 2, 4, 6, 8, 16\n",
    "\n",
    "Q = torch.randn(batch, n, d_k)\n",
    "K = torch.randn(batch, m, d_k)\n",
    "V = torch.randn(batch, m, d_v)\n",
    "\n",
    "# Your implementation\n",
    "our_output, our_weights = scaled_dot_product_attention(Q, K, V)\n",
    "\n",
    "# PyTorch reference (F.scaled_dot_product_attention)\n",
    "ref_output = F.scaled_dot_product_attention(Q, K, V)\n",
    "\n",
    "print(f\"Output shape: {our_output.shape}\")  # Should be (2, 4, 16)\n",
    "print(f\"Weights shape: {our_weights.shape}\")  # Should be (2, 4, 6)\n",
    "print(f\"Max absolute error: {(our_output - ref_output).abs().max().item():.2e}\")\n",
    "print(f\"Weights sum to 1 per row: {our_weights.sum(dim=-1)}\")\n",
    "\n",
    "# Sanity checks\n",
    "assert our_output.shape == (batch, n, d_v), f\"Expected ({batch}, {n}, {d_v}), got {our_output.shape}\"\n",
    "assert our_weights.shape == (batch, n, m), f\"Expected ({batch}, {n}, {m}), got {our_weights.shape}\"\n",
    "assert torch.allclose(our_weights.sum(dim=-1), torch.ones(batch, n), atol=1e-6), \"Weights don't sum to 1\"\n",
    "assert torch.allclose(our_output, ref_output, atol=1e-5), \"Output doesn't match PyTorch reference\"\n",
    "print(\"\\nAll checks passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment: Visualizing the Scaling Factor\n",
    "\n",
    "Empirical verification of why $\\sqrt{d_k}$ matters. For increasing $d_k$, compare the attention distributions with and without scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate the effect of scaling on softmax distributions\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "for idx, d_k in enumerate([8, 64, 512]):\n",
    "    q = torch.randn(1, 1, d_k)  # single query\n",
    "    K = torch.randn(1, 10, d_k)  # 10 keys\n",
    "\n",
    "    # Unscaled\n",
    "    scores_unscaled = (q @ K.transpose(-2, -1)).squeeze()\n",
    "    weights_unscaled = F.softmax(scores_unscaled, dim=-1).detach().numpy()\n",
    "\n",
    "    # Scaled\n",
    "    scores_scaled = (q @ K.transpose(-2, -1) / np.sqrt(d_k)).squeeze()\n",
    "    weights_scaled = F.softmax(scores_scaled, dim=-1).detach().numpy()\n",
    "\n",
    "    ax = axes[idx]\n",
    "    x = np.arange(10)\n",
    "    width = 0.35\n",
    "    ax.bar(x - width/2, weights_unscaled, width, label='Unscaled', alpha=0.8)\n",
    "    ax.bar(x + width/2, weights_scaled, width, label='Scaled', alpha=0.8)\n",
    "    ax.set_title(f'$d_k$ = {d_k}')\n",
    "    ax.set_xlabel('Key index')\n",
    "    ax.set_ylabel('Attention weight')\n",
    "    ax.legend()\n",
    "    ax.set_ylim(0, 1)\n",
    "\n",
    "plt.suptitle('Effect of $\\\\sqrt{d_k}$ scaling on attention distributions', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print variance of dot products to verify theory\n",
    "print(\"\\nEmpirical variance of dot products (theory predicts d_k):\")\n",
    "for d_k in [8, 64, 512]:\n",
    "    q = torch.randn(1000, d_k)\n",
    "    k = torch.randn(1000, d_k)\n",
    "    dots = (q * k).sum(dim=-1)\n",
    "    print(f\"  d_k={d_k:>3d}: Var(q*k) = {dots.var().item():.1f} (expected {d_k})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment: Masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Causal mask example — decoder self-attention\n",
    "seq_len = 5\n",
    "causal_mask = torch.tril(torch.ones(seq_len, seq_len)).unsqueeze(0)  # (1, 5, 5)\n",
    "print(\"Causal mask:\")\n",
    "print(causal_mask.squeeze())\n",
    "\n",
    "Q = torch.randn(1, seq_len, 16)\n",
    "K = torch.randn(1, seq_len, 16)\n",
    "V = torch.randn(1, seq_len, 16)\n",
    "\n",
    "_, weights_no_mask = scaled_dot_product_attention(Q, K, V)\n",
    "_, weights_causal = scaled_dot_product_attention(Q, K, V, mask=causal_mask)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 4))\n",
    "ax1.imshow(weights_no_mask[0].detach().numpy(), cmap='Blues')\n",
    "ax1.set_title('No mask (encoder)')\n",
    "ax1.set_xlabel('Key position')\n",
    "ax1.set_ylabel('Query position')\n",
    "\n",
    "ax2.imshow(weights_causal[0].detach().numpy(), cmap='Blues')\n",
    "ax2.set_title('Causal mask (decoder)')\n",
    "ax2.set_xlabel('Key position')\n",
    "ax2.set_ylabel('Query position')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connections\n",
    "\n",
    "**Self-attention vs. cross-attention:**\n",
    "- **Self-attention:** $Q, K, V$ are all linear projections of the same input $X$: $Q = XW^Q$, $K = XW^K$, $V = XW^V$. Used in BERT (bidirectional), GPT (causal).\n",
    "- **Cross-attention:** $Q$ comes from one sequence (e.g., decoder), $K, V$ from another (e.g., encoder output). Used in encoder-decoder models (T5, original Transformer decoder).\n",
    "\n",
    "**Quadratic bottleneck:**\n",
    "The attention matrix $A \\in \\mathbb{R}^{n \\times m}$ requires $O(n \\cdot m)$ computation and memory. For self-attention ($n = m = L$, sequence length), this is $O(L^2)$. This is the bottleneck that Flash Attention (IO-aware tiling), sparse attention (attend to subset of positions), and linear attention (kernel approximation of softmax) address.\n",
    "\n",
    "**Next notebook:** Multi-head attention splits $d_{\\text{model}}$ into $h$ heads of dimension $d_k = d_{\\text{model}} / h$, letting the model attend to different subspace representations simultaneously."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Equations\n",
    "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n",
    "\n",
    "**Dimensions:** $Q \\in \\mathbb{R}^{n \\times d_k}, \\; K \\in \\mathbb{R}^{m \\times d_k}, \\; V \\in \\mathbb{R}^{m \\times d_v} \\;\\Rightarrow\\; \\text{Output} \\in \\mathbb{R}^{n \\times d_v}$\n",
    "\n",
    "**Scaling:** If $q_i, k_j \\sim \\mathcal{N}(0, 1)$ i.i.d., then $\\text{Var}(q \\cdot k) = d_k$. Dividing by $\\sqrt{d_k}$ normalizes variance to 1, preventing softmax saturation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
