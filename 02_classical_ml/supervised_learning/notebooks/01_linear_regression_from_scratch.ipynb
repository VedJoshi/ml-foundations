{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression from Scratch\n",
    "\n",
    "This notebook implements linear regression from first principles, connecting every line of code to the mathematics.\n",
    "\n",
    "## Objectives\n",
    "1. Implement OLS using the normal equations\n",
    "2. Implement OLS using gradient descent\n",
    "3. Add L2 regularization (Ridge)\n",
    "4. Visualize the bias-variance tradeoff with polynomial regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "np.random.seed(42)\n",
    "plt.style.use('seaborn-v0_8-whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Generate Synthetic Data\n",
    "\n",
    "I'll create data from a known linear model plus noise:\n",
    "$$y = w^*{}^\\top x + \\varepsilon, \\quad \\varepsilon \\sim \\mathcal{N}(0, \\sigma^2)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_linear_data(n_samples=100, n_features=2, noise_std=0.5, w_true=None):\n",
    "    \"\"\"Generate data from linear model y = Xw + noise.\"\"\"\n",
    "    X = np.random.randn(n_samples, n_features)\n",
    "    \n",
    "    if w_true is None:\n",
    "        w_true = np.random.randn(n_features)\n",
    "    \n",
    "    y = X @ w_true + noise_std * np.random.randn(n_samples)\n",
    "    \n",
    "    return X, y, w_true\n",
    "\n",
    "# Generate data\n",
    "X, y, w_true = generate_linear_data(n_samples=200, n_features=3, noise_std=0.5)\n",
    "print(f\"X shape: {X.shape}\")\n",
    "print(f\"y shape: {y.shape}\")\n",
    "print(f\"True weights: {w_true}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Normal Equations Solution\n",
    "\n",
    "The closed-form solution:\n",
    "$$w^* = (X^\\top X)^{-1} X^\\top y$$\n",
    "\n",
    "I'll implement this, but use `np.linalg.solve` instead of explicitly computing the inverse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegressionNormalEq:\n",
    "    \"\"\"Linear regression using normal equations.\"\"\"\n",
    "    \n",
    "    def __init__(self, fit_intercept=True):\n",
    "        self.fit_intercept = fit_intercept\n",
    "        self.w = None\n",
    "        self.b = 0\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Fit using normal equations: w* = (X^T X)^{-1} X^T y\"\"\"\n",
    "        n, d = X.shape\n",
    "        \n",
    "        if self.fit_intercept:\n",
    "            # Add bias column\n",
    "            X_aug = np.column_stack([np.ones(n), X])\n",
    "        else:\n",
    "            X_aug = X\n",
    "        \n",
    "        # Normal equations: (X^T X) w = X^T y\n",
    "        # Using solve instead of inverse for numerical stability\n",
    "        XtX = X_aug.T @ X_aug\n",
    "        Xty = X_aug.T @ y\n",
    "        w_full = np.linalg.solve(XtX, Xty)\n",
    "        \n",
    "        if self.fit_intercept:\n",
    "            self.b = w_full[0]\n",
    "            self.w = w_full[1:]\n",
    "        else:\n",
    "            self.w = w_full\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return X @ self.w + self.b\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        \"\"\"R^2 score.\"\"\"\n",
    "        y_pred = self.predict(X)\n",
    "        ss_res = np.sum((y - y_pred)**2)\n",
    "        ss_tot = np.sum((y - y.mean())**2)\n",
    "        return 1 - ss_res / ss_tot\n",
    "\n",
    "# Fit and evaluate\n",
    "model_ne = LinearRegressionNormalEq(fit_intercept=False)\n",
    "model_ne.fit(X, y)\n",
    "\n",
    "print(f\"Estimated weights: {model_ne.w}\")\n",
    "print(f\"True weights:      {w_true}\")\n",
    "print(f\"R^2 score: {model_ne.score(X, y):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Gradient Descent Solution\n",
    "\n",
    "The gradient of MSE loss:\n",
    "$$\\nabla_w J = \\frac{2}{n} X^\\top (Xw - y)$$\n",
    "\n",
    "Update rule:\n",
    "$$w_{t+1} = w_t - \\alpha \\nabla_w J$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegressionGD:\n",
    "    \"\"\"Linear regression using gradient descent.\"\"\"\n",
    "    \n",
    "    def __init__(self, learning_rate=0.01, n_iterations=1000, fit_intercept=True):\n",
    "        self.lr = learning_rate\n",
    "        self.n_iter = n_iterations\n",
    "        self.fit_intercept = fit_intercept\n",
    "        self.w = None\n",
    "        self.b = 0\n",
    "        self.history = []\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        n, d = X.shape\n",
    "        \n",
    "        # Initialize weights\n",
    "        self.w = np.zeros(d)\n",
    "        self.b = 0\n",
    "        self.history = []\n",
    "        \n",
    "        for _ in range(self.n_iter):\n",
    "            # Predictions\n",
    "            y_pred = X @ self.w + self.b\n",
    "            \n",
    "            # Compute gradients\n",
    "            # J = (1/n) ||Xw - y||^2\n",
    "            # dJ/dw = (2/n) X^T (Xw - y)\n",
    "            residual = y_pred - y\n",
    "            dw = (2/n) * (X.T @ residual)\n",
    "            db = (2/n) * np.sum(residual) if self.fit_intercept else 0\n",
    "            \n",
    "            # Update\n",
    "            self.w -= self.lr * dw\n",
    "            self.b -= self.lr * db\n",
    "            \n",
    "            # Track loss\n",
    "            loss = np.mean(residual**2)\n",
    "            self.history.append(loss)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return X @ self.w + self.b\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        y_pred = self.predict(X)\n",
    "        ss_res = np.sum((y - y_pred)**2)\n",
    "        ss_tot = np.sum((y - y.mean())**2)\n",
    "        return 1 - ss_res / ss_tot\n",
    "\n",
    "# Fit and compare\n",
    "model_gd = LinearRegressionGD(learning_rate=0.1, n_iterations=500, fit_intercept=False)\n",
    "model_gd.fit(X, y)\n",
    "\n",
    "print(f\"GD weights:     {model_gd.w}\")\n",
    "print(f\"Normal Eq:      {model_ne.w}\")\n",
    "print(f\"True weights:   {w_true}\")\n",
    "print(f\"\\nClose match: {np.allclose(model_gd.w, model_ne.w, atol=1e-3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training loss\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(model_gd.history)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('MSE Loss')\n",
    "plt.title('Gradient Descent Training Loss')\n",
    "plt.yscale('log')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Ridge Regression (L2 Regularization)\n",
    "\n",
    "Regularized objective:\n",
    "$$J_{\\text{ridge}}(w) = ||y - Xw||^2 + \\lambda ||w||^2$$\n",
    "\n",
    "Closed-form solution:\n",
    "$$w^* = (X^\\top X + \\lambda I)^{-1} X^\\top y$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RidgeRegression:\n",
    "    \"\"\"Ridge regression with L2 regularization.\"\"\"\n",
    "    \n",
    "    def __init__(self, alpha=1.0):\n",
    "        self.alpha = alpha  # regularization strength\n",
    "        self.w = None\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        n, d = X.shape\n",
    "        \n",
    "        # (X^T X + λI) w = X^T y\n",
    "        XtX = X.T @ X\n",
    "        reg = self.alpha * np.eye(d)\n",
    "        Xty = X.T @ y\n",
    "        \n",
    "        self.w = np.linalg.solve(XtX + reg, Xty)\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return X @ self.w\n",
    "\n",
    "# Compare different regularization strengths\n",
    "alphas = [0, 0.1, 1, 10, 100]\n",
    "print(f\"True weights: {w_true}\\n\")\n",
    "\n",
    "for alpha in alphas:\n",
    "    ridge = RidgeRegression(alpha=alpha)\n",
    "    ridge.fit(X, y)\n",
    "    print(f\"α = {alpha:5.1f}: w = {ridge.w}, ||w|| = {np.linalg.norm(ridge.w):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation:** Higher λ shrinks weights toward zero, trading bias for reduced variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Polynomial Regression & Bias-Variance Tradeoff\n",
    "\n",
    "Demonstrate overfitting with polynomial features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def polynomial_features(X, degree):\n",
    "    \"\"\"Create polynomial features up to given degree.\n",
    "    \n",
    "    For 1D input: [1, x, x^2, ..., x^degree]\n",
    "    \"\"\"\n",
    "    X = np.asarray(X).ravel()\n",
    "    n = len(X)\n",
    "    features = np.ones((n, degree + 1))\n",
    "    for d in range(1, degree + 1):\n",
    "        features[:, d] = X ** d\n",
    "    return features\n",
    "\n",
    "# Generate 1D data with nonlinear pattern\n",
    "np.random.seed(42)\n",
    "n = 30\n",
    "X_1d = np.sort(np.random.uniform(-3, 3, n))\n",
    "y_1d = np.sin(X_1d) + 0.3 * np.random.randn(n)  # True function is sine\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_1d, y_1d, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit polynomials of different degrees\n",
    "degrees = [1, 3, 5, 9, 15]\n",
    "fig, axes = plt.subplots(1, len(degrees), figsize=(15, 3))\n",
    "\n",
    "train_errors = []\n",
    "test_errors = []\n",
    "\n",
    "X_plot = np.linspace(-3.5, 3.5, 100)\n",
    "\n",
    "for ax, degree in zip(axes, degrees):\n",
    "    # Create features\n",
    "    X_train_poly = polynomial_features(X_train, degree)\n",
    "    X_test_poly = polynomial_features(X_test, degree)\n",
    "    X_plot_poly = polynomial_features(X_plot, degree)\n",
    "    \n",
    "    # Fit\n",
    "    model = LinearRegressionNormalEq(fit_intercept=False)\n",
    "    model.fit(X_train_poly, y_train)\n",
    "    \n",
    "    # Predict\n",
    "    y_train_pred = model.predict(X_train_poly)\n",
    "    y_test_pred = model.predict(X_test_poly)\n",
    "    y_plot = model.predict(X_plot_poly)\n",
    "    \n",
    "    # Errors\n",
    "    train_mse = np.mean((y_train - y_train_pred)**2)\n",
    "    test_mse = np.mean((y_test - y_test_pred)**2)\n",
    "    train_errors.append(train_mse)\n",
    "    test_errors.append(test_mse)\n",
    "    \n",
    "    # Plot\n",
    "    ax.scatter(X_train, y_train, c='blue', alpha=0.6, label='Train')\n",
    "    ax.scatter(X_test, y_test, c='red', alpha=0.6, label='Test')\n",
    "    ax.plot(X_plot, y_plot, 'g-', linewidth=2)\n",
    "    ax.set_title(f'Degree {degree}\\nTrain: {train_mse:.3f}, Test: {test_mse:.3f}')\n",
    "    ax.set_ylim(-2, 2)\n",
    "    ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot train vs test error as function of complexity\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(degrees, train_errors, 'bo-', label='Train Error')\n",
    "plt.plot(degrees, test_errors, 'ro-', label='Test Error')\n",
    "plt.xlabel('Polynomial Degree')\n",
    "plt.ylabel('MSE')\n",
    "plt.title('Bias-Variance Tradeoff')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nBias-Variance Tradeoff:\")\n",
    "print(\"- Low degree: High bias (underfitting), low variance\")\n",
    "print(\"- High degree: Low bias, high variance (overfitting)\")\n",
    "print(\"- Optimal complexity minimizes test error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Exercises\n",
    "\n",
    "Complete these to solidify understanding:\n",
    "\n",
    "1. [ ] Implement mini-batch SGD for linear regression\n",
    "2. [ ] Add learning rate scheduling to GD and observe convergence\n",
    "3. [ ] Implement cross-validation to select optimal polynomial degree\n",
    "4. [ ] Compare scikit-learn's LinearRegression to your implementation\n",
    "5. [ ] Visualize the effect of Ridge regularization on polynomial overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **Normal equations** give closed-form solution when $X^\\top X$ is invertible\n",
    "2. **Gradient descent** is iterative but generalizes to other loss functions\n",
    "3. **Ridge regression** adds $\\lambda ||w||^2$ penalty, making $(X^\\top X + \\lambda I)$ always invertible\n",
    "4. **Polynomial features** demonstrate bias-variance tradeoff\n",
    "5. **Always evaluate on held-out data** to detect overfitting"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
